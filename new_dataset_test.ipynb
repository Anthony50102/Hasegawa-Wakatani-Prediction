{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, h5_file_path: str,):\n",
    "        self._load_data(h5_file_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 40001\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a specific chunk of data\n",
    "        Ideally we want to be given a slice of the data that we want to grab\n",
    "        Returns:\n",
    "            tuple: (input_data, targets)\n",
    "            - input_data: tensor of shape (channels, x_dim, y_dim, chunk_size)\n",
    "            - targets: dictionary of invariants for this chunk\n",
    "        \"\"\"\n",
    "        data = self.inputs['data'][index]\n",
    "        gamma_n = self.inputs['gamma_n'][index]\n",
    "        return {'data': data, 'gamma_n': gamma_n}\n",
    "    \n",
    "    def _load_h5_file_with_data(self, file_path:str, derived_data_key:str = \"invariants\"):\n",
    "        \"\"\"Method for loading .h5 files\n",
    "        \n",
    "        :returns: dict that contains name of the .h5 file as stored in the .h5 file, as well as a generator of the data\n",
    "        \"\"\"\n",
    "        file = h5py.File(file_path)\n",
    "        key = list(file.keys())[0]\n",
    "        data = file[key]\n",
    "        derived_data = file[derived_data_key]\n",
    "        gamma_c = file[derived_data_key]['$\\\\Gamma_c$']\n",
    "        gamma_n = file[derived_data_key]['$\\\\Gamma_n$']\n",
    "        E = file[derived_data_key]['$\\\\mathcal{D}^E$']\n",
    "        U = file[derived_data_key]['$\\\\mathcal{D}^U$']\n",
    "        energy = file[derived_data_key]['energy']\n",
    "        enstrophy = file[derived_data_key]['enstrophy']\n",
    "        time = file[derived_data_key]['time']\n",
    "        return dict(file=file, data=data, gamma_c=gamma_c, gamma_n=gamma_n, E=E, U=U, energy=energy, enstrophy=enstrophy, time=time)\n",
    "    \n",
    "    def _load_data(self, input_file, target_files=None):\n",
    "        \"\"\"Loads input data and optional target data into the dataset\n",
    "        Args:\n",
    "            input_file (str): Name of the input .h5 file\n",
    "            target_files (dict): Dictionary mapping task names to target .h5 file names\n",
    "        \"\"\"\n",
    "        self.inputs = self._load_h5_file_with_data(input_file) #Dictionary with keys \"file\", \"data\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Indexing elements must be in increasing order",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sampler \u001b[38;5;241m=\u001b[39m SubsetRandomSampler(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m40001\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampler\u001b[38;5;241m=\u001b[39m BatchSampler(sampler, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/hw/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/hw/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Get a specific chunk of data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Ideally we want to be given a slice of the data that we want to grab\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        - targets: dictionary of invariants for this chunk\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m     gamma_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma_n\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma_n\u001b[39m\u001b[38;5;124m'\u001b[39m: gamma_n}\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw/lib/python3.10/site-packages/h5py/_hl/dataset.py:854\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# === Everything else ===================\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# Perform the dataspace selection.\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m selection \u001b[38;5;241m=\u001b[39m \u001b[43msel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selection\u001b[38;5;241m.\u001b[39mnselect \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mzeros(selection\u001b[38;5;241m.\u001b[39marray_shape, dtype\u001b[38;5;241m=\u001b[39mnew_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/hw/lib/python3.10/site-packages/h5py/_hl/selections.py:82\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(shape, args, dataset)\u001b[0m\n\u001b[1;32m     79\u001b[0m     space \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape)\n\u001b[1;32m     80\u001b[0m     selector \u001b[38;5;241m=\u001b[39m _selector\u001b[38;5;241m.\u001b[39mSelector(space)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mh5py/_selector.pyx:282\u001b[0m, in \u001b[0;36mh5py._selector.Selector.make_selection\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_selector.pyx:215\u001b[0m, in \u001b[0;36mh5py._selector.Selector.apply_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Indexing elements must be in increasing order"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import SubsetRandomSampler, DataLoader, BatchSampler\n",
    "dataset = CustomDataset(\"/Users/anthonypoole/Repositories/hw_snapshots.h5\")\n",
    "sampler = SubsetRandomSampler(torch.arange(0,40001,1).tolist())\n",
    "loader = DataLoader(dataset=dataset, batch_size=None, sampler= BatchSampler(sampler, 32, True))\n",
    "for item in loader:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
